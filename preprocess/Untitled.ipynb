{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33778e5c-6fa0-422a-8293-6f2ca0af02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame 339\n",
      "✅ Finished processing video and matte with consistent bokeh.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- Path Setup -----\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from midas.midas.dpt_depth import DPTDepthModel\n",
    "from midas.midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "# ---------- Load MiDaS Model ----------\n",
    "def load_midas_model():\n",
    "    model = DPTDepthModel(\"../midas/midas/weights/dpt_large_384.pt\", pretrained=True, backbone=\"vitl16_384\", non_negative=True)\n",
    "    model.eval().cuda()\n",
    "    transform = Compose([\n",
    "        Resize(384, 384, keep_aspect_ratio=True),\n",
    "        NormalizeImage(mean=[0.5]*3, std=[0.5]*3),\n",
    "        PrepareForNet()\n",
    "    ])\n",
    "    return model, transform\n",
    "\n",
    "# ---------- Estimate Depth Map ----------\n",
    "def get_depth_map(img, model, transform):\n",
    "    sample = transform({\"image\": img / 255.0})\n",
    "    input_tensor = sample[\"image\"]\n",
    "    if isinstance(input_tensor, np.ndarray):\n",
    "        input_tensor = torch.from_numpy(input_tensor)\n",
    "    input_tensor = input_tensor.unsqueeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)[0]\n",
    "\n",
    "    depth_map = prediction.cpu().numpy()\n",
    "    depth_map = cv2.resize(depth_map, (img.shape[1], img.shape[0]))\n",
    "    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
    "    return depth_map\n",
    "\n",
    "# ---------- Compute Focus from Alpha ----------\n",
    "def compute_focus_from_alpha(alpha_gray, depth):\n",
    "    mask = alpha_gray > 127\n",
    "    if np.sum(mask) == 0:\n",
    "        return 0.5  # fallback\n",
    "    return float(np.mean(depth[mask]))\n",
    "\n",
    "# ---------- Apply Depth-Aware Bokeh with Foreground Protection ----------\n",
    "def apply_depth_bokeh(img, depth_map, focus=0.4, max_blur=15, alpha_mask=None):\n",
    "    output = np.zeros_like(img, dtype=np.float32)\n",
    "    steps = 8\n",
    "    focus_range = 0.05\n",
    "\n",
    "    for i in range(steps):\n",
    "        blur_strength = int((i / (steps - 1)) * max_blur)\n",
    "        if blur_strength % 2 == 0:\n",
    "            blur_strength += 1\n",
    "\n",
    "        blurred = cv2.GaussianBlur(img, (blur_strength, blur_strength), 0)\n",
    "        lower = focus + (i - steps // 2) / steps\n",
    "        upper = focus + (i + 1 - steps // 2) / steps\n",
    "\n",
    "        mask = (depth_map >= lower) & (depth_map < upper)\n",
    "        mask = cv2.GaussianBlur(mask.astype(np.float32), (11, 11), 0)\n",
    "\n",
    "        for c in range(img.shape[2]):\n",
    "            output[..., c] += blurred[..., c].astype(np.float32) * mask\n",
    "\n",
    "    # Foreground preservation using alpha matte\n",
    "    if alpha_mask is not None:\n",
    "        alpha_norm = alpha_mask.astype(np.float32) / 255.0\n",
    "        for c in range(img.shape[2]):\n",
    "            output[..., c] = alpha_norm * img[..., c] + (1 - alpha_norm) * output[..., c]\n",
    "\n",
    "    return np.clip(output, 0, 255).astype(np.uint8)\n",
    "\n",
    "# ---------- Process Video with Matching Bokeh ----------\n",
    "def process_video_with_matte(input_video_path, alpha_video_path, output_rgb_path, output_alpha_path):\n",
    "    cap_rgb = cv2.VideoCapture(input_video_path)\n",
    "    cap_alpha = cv2.VideoCapture(alpha_video_path)\n",
    "\n",
    "    frame_width = int(cap_rgb.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap_rgb.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap_rgb.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    out_rgb = cv2.VideoWriter(output_rgb_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    out_alpha = cv2.VideoWriter(output_alpha_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height), isColor=False)\n",
    "\n",
    "    model, transform = load_midas_model()\n",
    "    smoothed_depth = None\n",
    "    alpha_smooth = 0.8\n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret_rgb, frame = cap_rgb.read()\n",
    "        ret_alpha, alpha = cap_alpha.read()\n",
    "        if not ret_rgb or not ret_alpha:\n",
    "            break\n",
    "\n",
    "        alpha_gray = cv2.cvtColor(alpha, cv2.COLOR_BGR2GRAY)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        current_depth = get_depth_map(frame_rgb, model, transform)\n",
    "\n",
    "        if smoothed_depth is None:\n",
    "            smoothed_depth = current_depth\n",
    "        else:\n",
    "            smoothed_depth = alpha_smooth * smoothed_depth + (1 - alpha_smooth) * current_depth\n",
    "\n",
    "        depth = smoothed_depth\n",
    "        focus = compute_focus_from_alpha(alpha_gray, depth)\n",
    "\n",
    "        # ✅ Apply bokeh to RGB with alpha protection\n",
    "        blurred_frame = apply_depth_bokeh(frame, depth, focus=focus, alpha_mask=alpha_gray)\n",
    "\n",
    "        # ✅ Apply bokeh to matte with same protection\n",
    "        alpha_color = cv2.cvtColor(alpha_gray, cv2.COLOR_GRAY2BGR)\n",
    "        blurred_alpha = apply_depth_bokeh(alpha_color, depth, focus=focus, alpha_mask=alpha_gray)\n",
    "        blurred_alpha_gray = cv2.cvtColor(blurred_alpha, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        out_rgb.write(blurred_frame)\n",
    "        out_alpha.write(blurred_alpha_gray)\n",
    "\n",
    "        frame_idx += 1\n",
    "        print(f\"Processed frame {frame_idx}\", end='\\r')\n",
    "\n",
    "    cap_rgb.release()\n",
    "    cap_alpha.release()\n",
    "    out_rgb.release()\n",
    "    out_alpha.release()\n",
    "    print(\"\\n✅ Finished processing video and matte with consistent bokeh.\")\n",
    "\n",
    "# Example usage:\n",
    "process_video_with_matte(\n",
    "    \"../data/VideoMatte240K/train/fgr/0000.mp4\",\n",
    "    \"../data/VideoMatte240K/train/pha/0000.mp4\",\n",
    "    \"output_blurred_rgb.mp4\",\n",
    "    \"output_blurred_alpha.mp4\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "807b253b-c74c-43d8-bbd0-26e6facdcd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Generating depth map from reference frame...\n",
      "🌀 Creating static blur maps...\n",
      "Processed frame 339\n",
      "✅ Done: Full-subject blur + matching soft matte, flicker-free.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- Path Setup -----\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from midas.midas.dpt_depth import DPTDepthModel\n",
    "from midas.midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "# ---------- Load MiDaS ----------\n",
    "def load_midas_model():\n",
    "    model = DPTDepthModel(\n",
    "        \"../midas/midas/weights/dpt_large_384.pt\",\n",
    "        pretrained=True,\n",
    "        backbone=\"vitl16_384\",\n",
    "        non_negative=True\n",
    "    )\n",
    "    model.eval().cuda()\n",
    "    transform = Compose([\n",
    "        Resize(384, 384, keep_aspect_ratio=True),\n",
    "        NormalizeImage(mean=[0.5]*3, std=[0.5]*3),\n",
    "        PrepareForNet()\n",
    "    ])\n",
    "    return model, transform\n",
    "\n",
    "# ---------- Get Depth Map ----------\n",
    "def get_depth_map(img, model, transform):\n",
    "    sample = transform({\"image\": img / 255.0})\n",
    "    input_tensor = sample[\"image\"]\n",
    "    if isinstance(input_tensor, np.ndarray):\n",
    "        input_tensor = torch.from_numpy(input_tensor)\n",
    "    input_tensor = input_tensor.unsqueeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)[0]\n",
    "\n",
    "    depth_map = prediction.cpu().numpy()\n",
    "    depth_map = cv2.resize(depth_map, (img.shape[1], img.shape[0]))\n",
    "    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
    "    return depth_map\n",
    "\n",
    "# ---------- Create Static Blur Map ----------\n",
    "def create_static_blur_map(depth_map, focus_mask, focus_range=0.02, steps=8, max_blur=65):\n",
    "    subject_depth = np.mean(depth_map[focus_mask > 127])\n",
    "    depth_diff = np.abs(depth_map - subject_depth)\n",
    "    blur_levels = np.clip((depth_diff - focus_range) / (1 - focus_range), 0, 1)\n",
    "\n",
    "    blur_maps = []\n",
    "    for i in range(steps):\n",
    "        blur_strength = int((i / (steps - 1)) * max_blur)\n",
    "        if blur_strength % 2 == 0:\n",
    "            blur_strength += 1\n",
    "        mask = np.clip(1 - np.abs(blur_levels - i / (steps - 1)) * steps, 0, 1)\n",
    "        mask = cv2.GaussianBlur(mask.astype(np.float32), (15, 15), 0)\n",
    "        blur_maps.append((blur_strength, mask))\n",
    "\n",
    "    return blur_maps\n",
    "\n",
    "# ---------- Apply Precomputed Blur Map ----------\n",
    "def apply_precomputed_blur(image, blur_maps):\n",
    "    is_gray = image.ndim == 2\n",
    "    if is_gray:\n",
    "        image = image[..., np.newaxis]\n",
    "\n",
    "    output = np.zeros_like(image, dtype=np.float32)\n",
    "\n",
    "    for blur_strength, mask in blur_maps:\n",
    "        blurred = cv2.GaussianBlur(image, (blur_strength, blur_strength), 0)\n",
    "        if blurred.ndim == 2:\n",
    "            blurred = blurred[..., np.newaxis]\n",
    "        for c in range(image.shape[2]):\n",
    "            output[..., c] += blurred[..., c] * mask\n",
    "\n",
    "    output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "    return output.squeeze() if is_gray else output\n",
    "\n",
    "# ---------- Process Video ----------\n",
    "def process_video_static_blur_mask(input_rgb, input_alpha, output_rgb, output_alpha):\n",
    "    cap_rgb = cv2.VideoCapture(input_rgb)\n",
    "    cap_alpha = cv2.VideoCapture(input_alpha)\n",
    "\n",
    "    frame_width = int(cap_rgb.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap_rgb.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap_rgb.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    out_rgb = cv2.VideoWriter(output_rgb, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    out_alpha = cv2.VideoWriter(output_alpha, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height), isColor=False)\n",
    "\n",
    "    model, transform = load_midas_model()\n",
    "\n",
    "    # --- Use FIRST frame to generate static blur maps ---\n",
    "    ret_rgb, ref_frame = cap_rgb.read()\n",
    "    ret_alpha, ref_alpha = cap_alpha.read()\n",
    "    if not ret_rgb or not ret_alpha:\n",
    "        print(\"Error reading reference frame\")\n",
    "        return\n",
    "\n",
    "    ref_rgb_input = cv2.cvtColor(ref_frame, cv2.COLOR_BGR2RGB)\n",
    "    ref_alpha_gray = cv2.cvtColor(ref_alpha, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    print(\"🔍 Generating depth map from reference frame...\")\n",
    "    depth_map = get_depth_map(ref_rgb_input, model, transform)\n",
    "\n",
    "    print(\"🌀 Creating static blur maps...\")\n",
    "    blur_maps = create_static_blur_map(depth_map, ref_alpha_gray)\n",
    "\n",
    "    # Rewind\n",
    "    cap_rgb.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    cap_alpha.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    # --- Process all frames using same blur maps ---\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret_rgb, frame = cap_rgb.read()\n",
    "        ret_alpha, alpha = cap_alpha.read()\n",
    "        if not ret_rgb or not ret_alpha:\n",
    "            break\n",
    "\n",
    "        alpha_gray = cv2.cvtColor(alpha, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        blurred_rgb = apply_precomputed_blur(frame, blur_maps)\n",
    "        blurred_alpha = apply_precomputed_blur(alpha_gray, blur_maps)\n",
    "\n",
    "        out_rgb.write(blurred_rgb)\n",
    "        out_alpha.write(blurred_alpha)\n",
    "\n",
    "        frame_idx += 1\n",
    "        print(f\"Processed frame {frame_idx}\", end='\\r')\n",
    "\n",
    "    cap_rgb.release()\n",
    "    cap_alpha.release()\n",
    "    out_rgb.release()\n",
    "    out_alpha.release()\n",
    "    print(\"\\n✅ Done: Full-subject blur + matching soft matte, flicker-free.\")\n",
    "\n",
    "# ✅ RUN IT\n",
    "process_video_static_blur_mask(\n",
    "    \"../data/VideoMatte240K/train/fgr/0000.mp4\",     # Your RGB video\n",
    "    \"../data/VideoMatte240K/train/pha/0000.mp4\",     # Your alpha matte video\n",
    "    \"blurred_rgb_subject_and_bg.mp4\",                # Output RGB\n",
    "    \"blurred_alpha_soft_subject_mask.mp4\"            # Output soft matte\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583d865-eb9c-4160-95a2-254dd32ee1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (defocused-env)",
   "language": "python",
   "name": "defocused-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
